{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.dammit import EncodingDetector\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tables(html_table, tag=['th', 'tr', 'td']):\n",
    "    table_value = []\n",
    "\n",
    "\n",
    "    # for my_table in tables:\n",
    "\n",
    "    # You can find children with multiple tags by passing a list of strings\n",
    "    rows = html_table.findChildren(tag)\n",
    "\n",
    "    for row in rows:\n",
    "        cells = row.findChildren(tag)\n",
    "        for cell in cells:\n",
    "            value = cell.string\n",
    "            if value:\n",
    "                table_value.append(value.strip())\n",
    "                # print(\"The value in this cell is %s\" % value)\n",
    "            else:\n",
    "                table_value.append(\"None\")\n",
    "    return dict(zip(table_value[::2], table_value[1::2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page(url):\n",
    "    \n",
    "    # get the html file\n",
    "    page = requests.get(url)\n",
    "    \n",
    "    if page.status_code == 200:\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        # get all the tables\n",
    "        tables = soup.findChildren('table')\n",
    "\n",
    "        # only the first 4 tables are useful\n",
    "        tables = tables[:4]\n",
    "\n",
    "\n",
    "        # UPPER LEFT TABLE\n",
    "        table_ul = get_tables(tables[0])\n",
    "\n",
    "        # UPPER RIGHT TABLE\n",
    "        rows_html = tables[1].findAll(\"span\", {\"class\": lambda x: x and x.startswith(\"rating\")})\n",
    "        rows = [x.get_text() for x in rows_html]\n",
    "        table_ur = dict(zip(rows[::2], rows[1::2]))\n",
    "\n",
    "#         # BUTTOM TABLES\n",
    "\n",
    "#         table_b = {}\n",
    "#         for t in tables[2:]:\n",
    "#             l = get_tables(t, ['td', 'p', 'tr'])\n",
    "#             table_b.update(l)\n",
    "\n",
    "\n",
    "        all_items = {**table_ul, **table_ur}\n",
    "    \n",
    "    else:\n",
    "        print(page.status_code)\n",
    "\n",
    "\n",
    "    return all_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [04:40<00:00,  2.81s/it]\n"
     ]
    }
   ],
   "source": [
    "all_boards = []\n",
    "\n",
    "for page in tqdm(range(100)):\n",
    "    base_url = \"https://thegoodride.com/snowboard-reviews/?mens={}\".format(page)\n",
    "\n",
    "    parser = 'html.parser'  # or 'lxml' (preferred) or 'html5lib', if installed\n",
    "    resp = requests.get(base_url, headers={\"content-type\":\"text\"})\n",
    "    \n",
    "    if resp.status_code == 200:\n",
    "        http_encoding = resp.encoding if 'charset' in resp.headers.get('content-type', '').lower() else None\n",
    "        html_encoding = EncodingDetector.find_declared_encoding(resp.content, is_html=True)\n",
    "        encoding = html_encoding or http_encoding\n",
    "        soup = BeautifulSoup(resp.content, parser, from_encoding=encoding)\n",
    "\n",
    "        rows_html = soup.findAll(\"div\", {\"class\": \"board-reviews animate\"})\n",
    "\n",
    "\n",
    "        for board in rows_html:\n",
    "            board_name = board.select('h4')[0].text.strip()\n",
    "            review_url = board.select('a', href=True)[0]['href']\n",
    "            all_boards.append([board_name, review_url])\n",
    "    else:\n",
    "        print(\"ERROR {}\".format(resp.status_code))\n",
    "    \n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_url = pd.DataFrame(all_boards, columns=['board_name', 'url'])\n",
    "df_url.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_url.to_csv('../data/100_pages.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: get date from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 71/4000 [03:09<1:34:09,  1.44s/it]"
     ]
    }
   ],
   "source": [
    "rating_list = []\n",
    "for url in tqdm(df_url['url']):\n",
    "    d = parse_page(url)\n",
    "    assert len(d) == 18\n",
    "    rating_list.append(d)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rating = pd.DataFrame(rating_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([df_url, df_rating], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
